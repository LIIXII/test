{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Created on Mon Sep 23 17:50:04 2013.\n",
    "Modified on Mon Nov 4 21:09:38 2019 by mozharovskyi.\n",
    "\n",
    "@author: baskiotis, salmon, gramfort\n",
    "\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "#               Impoort part\n",
    "###############################################################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from matplotlib import rc\n",
    "\n",
    "###############################################################################\n",
    "# Displaying labeled data\n",
    "###############################################################################\n",
    "\n",
    "symlist = ['o', 'p', '*', 's', '+', 'x', 'D', 'v', '-', '^']\n",
    "\n",
    "rc('font', **{'family': 'sans-serif', 'sans-serif': ['Computer Modern Roman']})\n",
    "params = {'axes.labelsize': 12,\n",
    "          'font.size': 16,\n",
    "          'legend.fontsize': 16,\n",
    "          'text.usetex': False,\n",
    "          'figure.figsize': (8, 6)}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "sns.set_context(\"poster\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set_style(\"white\")\n",
    "sns.axes_style()\n",
    "\n",
    "###############################################################################\n",
    "#    Data Generation    (you can skip the understanding)\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def rand_gauss(n=100, mu=[1, 1], sigmas=[0.1, 0.1]):\n",
    "    \"\"\"Sample  points from a Gaussian variable.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : number of samples\n",
    "\n",
    "    mu : centered\n",
    "\n",
    "    sigma : standard deviation\n",
    "    \"\"\"\n",
    "    d = len(mu)\n",
    "    res = np.random.randn(n, d)\n",
    "    return np.array(mu + res * sigmas)\n",
    "\n",
    "\n",
    "def rand_bi_gauss(n1=100, n2=100, mu1=[1, 1], mu2=[-1, -1], sigmas1=[0.1, 0.1],\n",
    "                  sigmas2=[0.1, 0.1]):\n",
    "    \"\"\"Sample points from two Gaussian distributions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : number of sample from first distribution\n",
    "\n",
    "    n2 : number of sample from second distribution\n",
    "\n",
    "    mu1 : center for first distribution\n",
    "\n",
    "    mu2 : center for second distribution\n",
    "\n",
    "    sigma1: std deviation for first distribution\n",
    "\n",
    "    sigma2: std deviation for second distribution\n",
    "    \"\"\"\n",
    "    ex1 = rand_gauss(n1, mu1, sigmas1)\n",
    "    ex2 = rand_gauss(n2, mu2, sigmas2)\n",
    "    y = np.hstack([np.ones(n1), -1 * np.ones(n2)])\n",
    "    X = np.vstack([ex1, ex2])\n",
    "    ind = np.random.permutation(n1 + n2)\n",
    "    return X[ind, :], y[ind]\n",
    "\n",
    "\n",
    "def rand_clown(n1=100, n2=100, sigma1=1, sigma2=2):\n",
    "    \"\"\"Create samples and labels form a **clown** dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : number of sample from first blob\n",
    "\n",
    "    n2 : number of sample from second blob\n",
    "\n",
    "    sigma1 :  noise std deviation for the first blob\n",
    "\n",
    "    sigma2 :  noise std deviation for the second blob\n",
    "    \"\"\"\n",
    "    x0 = np.random.randn(n1, 1)\n",
    "    x1 = x0 * x0 + sigma1 * np.random.randn(n1, 1)\n",
    "    x2 = np.hstack([sigma2 * np.random.randn(n2, 1),\n",
    "                    sigma2 * np.random.randn(n2, 1) + 2.])\n",
    "    X = np.vstack([np.hstack([x0, x1]), x2])\n",
    "    y = np.hstack([np.ones(n1), -1 * np.ones(n2)])\n",
    "    ind = np.random.permutation(n1 + n2)\n",
    "    return X[ind, :], y[ind]\n",
    "\n",
    "\n",
    "def rand_checkers(n1=100, n2=100, sigma=0.1):\n",
    "    \"\"\"Create samples and labels from a noisy checker.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n1 : number of samples for the first class\n",
    "\n",
    "    n2 : number of samples for the second class\n",
    "    \"\"\"\n",
    "    nbp = int(np.floor(n1 / 8))\n",
    "    nbn = int(np.floor(n2 / 8))\n",
    "    xapp = np.reshape(np.random.rand((nbp + nbn) * 16), [(nbp + nbn) * 8, 2])\n",
    "    yapp = np.ones((nbp + nbn) * 8)\n",
    "    idx = 0\n",
    "    for i in range(-2, 2):\n",
    "        for j in range(-2, 2):\n",
    "            if (((i + j) % 2) == 0):\n",
    "                nb = nbp\n",
    "            else:\n",
    "                nb = nbn\n",
    "                yapp[idx:(idx + nb)] = [-1] * nb\n",
    "\n",
    "            xapp[idx:(idx + nb), 0] = np.random.rand(nb)\n",
    "            xapp[idx:(idx + nb), 0] += i + sigma * np.random.randn(nb)\n",
    "            xapp[idx:(idx + nb), 1] = np.random.rand(nb)\n",
    "            xapp[idx:(idx + nb), 1] += j + sigma * np.random.randn(nb)\n",
    "            idx += nb\n",
    "\n",
    "    ind = np.random.permutation((nbp + nbn) * 8)\n",
    "    res = np.hstack([xapp, yapp[:, np.newaxis]])\n",
    "    return np.array(res[ind, :2]), np.array(res[ind, 2])\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#            Displaying labeled data\n",
    "###############################################################################\n",
    "symlist = ['o', 's', '+', 'x', 'D', '*', 'p', 'v', '-', '^']\n",
    "collist = ['blue', 'red', 'purple', 'orange', 'salmon', 'black', 'grey',\n",
    "           'fuchsia']\n",
    "\n",
    "\n",
    "def plot_2d(X, y, w=None, step=50, alpha_choice=1):\n",
    "    \"\"\"2D dataset data ploting according to labels.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    X : data features\n",
    "\n",
    "    y : label vector\n",
    "\n",
    "    w :(optional) the separating hyperplan w\n",
    "\n",
    "    alpha_choice : control alpha display parameter\n",
    "    \"\"\"\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    labels = np.unique(y)\n",
    "    k = np.unique(y).shape[0]\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", k)\n",
    "    sns.set_palette(color_blind_list)\n",
    "    for i, label in enumerate(y):\n",
    "        label_num = np.where(labels == label)[0][0]\n",
    "        plt.scatter(X[i, 0], X[i, 1],\n",
    "                    c=np.reshape(color_blind_list[label_num], (1, -1)),\n",
    "                    s=80, marker=symlist[label_num])\n",
    "    plt.xlim([min_tot0 - delta0 / 10., max_tot0 + delta0 / 10.])\n",
    "    plt.ylim([min_tot1 - delta1 / 10., max_tot1 + delta1 / 10.])\n",
    "    if w is not None:\n",
    "        plt.plot([min_tot0, max_tot0],\n",
    "                 [min_tot0 * -w[1] / w[2] - w[0] / w[2],\n",
    "                  max_tot0 * -w[1] / w[2] - w[0] / w[2]],\n",
    "                 \"k\", alpha=alpha_choice)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#               Loss functions and their gradient\n",
    "###############################################################################\n",
    "\n",
    "def predict(x, w):\n",
    "    \"\"\"Prediction from a normal vector.\"\"\"\n",
    "    return np.dot(x, w[1:]) + w[0]\n",
    "\n",
    "\n",
    "def predict_class(x, w):\n",
    "    \"\"\"Predict a class from at point x thanks to a normal vector.\"\"\"\n",
    "    return np.sign(predict(x, w))\n",
    "\n",
    "\n",
    "def zero_one_loss(x, y, w):\n",
    "    \"\"\"0-1 loss function.\"\"\"\n",
    "    return abs(y - np.sign(predict(x, w))) / 2.\n",
    "\n",
    "\n",
    "def hinge_loss(x, y, w):\n",
    "    \"\"\"Hinge loss function.\"\"\"\n",
    "    return np.maximum(0., 1. - y * predict(x, w))\n",
    "\n",
    "\n",
    "def mse_loss(x, y, w):\n",
    "    \"\"\"Mean square error loss.\"\"\"\n",
    "    return (y - predict(x, w)) ** 2\n",
    "\n",
    "\n",
    "def norm2(x, y, w):\n",
    "    \"\"\"Squared norm of a vector.\"\"\"\n",
    "    return np.dot(w, w)\n",
    "\n",
    "\n",
    "def gr_hinge_loss(x, y, w):\n",
    "    \"\"\"Sub-gradient of the loss function hingeloss.\"\"\"\n",
    "    return np.dot(-y * (hinge_loss(x, y, w) > 0.),\n",
    "                  np.hstack((np.ones((x.shape[0], 1)), x)))\n",
    "\n",
    "\n",
    "def gr_mse_loss(x, y, w):\n",
    "    \"\"\"Gradient of the least squares lost function.\"\"\"\n",
    "    return -2. * np.dot(y - predict(x, w),\n",
    "                        np.hstack((np.ones((x.shape[0], 1)), x)))\n",
    "\n",
    "\n",
    "def gr_norm2(x, y, w):\n",
    "    \"\"\"Gradient of the squared norm.\"\"\"\n",
    "    return 2. * w\n",
    "\n",
    "\n",
    "def pen_loss_aux(x, y, w, l):\n",
    "    \"\"\"Loss function penalized by hinge loss.\"\"\"\n",
    "    return hinge_loss(x, y, w) + l * norm2(x, y, w)\n",
    "\n",
    "\n",
    "def gr_pen_loss_aux(x, y, w, l):\n",
    "    \"\"\"Gradient of hinge loss penalized loss function.\"\"\"\n",
    "    return gr_hinge_loss(x, y, w) + l * gr_norm2(x, y, w, )\n",
    "\n",
    "\n",
    "def pen_loss(l):\n",
    "    \"\"\"Penalized loss function.\"\"\"\n",
    "    return lambda x, y, w: pen_loss_aux(x, y, w, l)\n",
    "\n",
    "\n",
    "def gr_pen_loss(l):\n",
    "    \"\"\"Gradient penalized loss function.\"\"\"\n",
    "    return lambda x, y, w: gr_pen_loss_aux(x, y, w, l)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#            Displaying tools for the Frontiere\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def frontiere(f, X, step=50, cmap_choice=cm.coolwarm):\n",
    "    \"\"\"Frontiere plotting for a decision function f.\"\"\"\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    xx, yy = np.meshgrid(np.arange(min_tot0, max_tot0, delta0 / step),\n",
    "                         np.arange(min_tot1, max_tot1, delta1 / step))\n",
    "    z = np.array([f(vec) for vec in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    z = z.reshape(xx.shape)\n",
    "    plt.imshow(z, origin='lower', interpolation=\"nearest\", cmap=cmap_choice,\n",
    "               extent=[min_tot0, max_tot0, min_tot1, max_tot1])\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "def frontiere_new(clf, X, y, w=None, step=50, alpha_choice=1, colorbar=True,\n",
    "                  samples=True, n_labels=3, n_neighbors=3):\n",
    "    \"\"\"Trace la frontiere pour la fonction de decision de clf.\"\"\"\n",
    "    min_tot0 = np.min(X[:, 0])\n",
    "    min_tot1 = np.min(X[:, 1])\n",
    "\n",
    "    max_tot0 = np.max(X[:, 0])\n",
    "    max_tot1 = np.max(X[:, 1])\n",
    "    delta0 = (max_tot0 - min_tot0)\n",
    "    delta1 = (max_tot1 - min_tot1)\n",
    "    xx, yy = np.meshgrid(np.arange(min_tot0, max_tot0, delta0 / step),\n",
    "                         np.arange(min_tot1, max_tot1, delta1 / step))\n",
    "    XX = np.c_[xx.ravel(), yy.ravel()]\n",
    "    print(XX.shape)\n",
    "    z = clf.predict(XX)\n",
    "    z = z.reshape(xx.shape)\n",
    "    labels = np.unique(z)\n",
    "    color_blind_list = sns.color_palette(\"colorblind\", labels.shape[0])\n",
    "    my_cmap = ListedColormap(color_blind_list)\n",
    "    plt.imshow(z, origin='lower', interpolation=\"mitchell\", alpha=0.80,\n",
    "               cmap=my_cmap, extent=[min_tot0, max_tot0, min_tot1, max_tot1])\n",
    "    if colorbar is True:\n",
    "        ax = plt.gca()\n",
    "        cbar = plt.colorbar(ticks=labels)\n",
    "        cbar.ax.set_yticklabels(labels)\n",
    "\n",
    "    # color_blind_list = sns.color_palette(\"colorblind\", labels.shape[0])\n",
    "    # sns.set_palette(color_blind_list)\n",
    "    ax = plt.gca()\n",
    "    if samples is True:\n",
    "        for i, label in enumerate(y):\n",
    "            label_num = np.where(labels == label)[0][0]\n",
    "            plt.scatter(X[i, 0], X[i, 1], c=color_blind_list[label_num],\n",
    "                        s=80, marker=symlist[label_num])\n",
    "    plt.xlim([min_tot0, max_tot0])\n",
    "    plt.ylim([min_tot1, max_tot1])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    if w is not None:\n",
    "        plt.plot([min_tot0, max_tot0],\n",
    "                 [min_tot0 * -w[1] / w[2] - w[0] / w[2],\n",
    "                  max_tot0 * -w[1] / w[2] - w[0] / w[2]],\n",
    "                 \"k\", alpha=alpha_choice)\n",
    "    plt.title(\"L=\" + str(n_labels) + \",k=\" +\n",
    "              str(n_neighbors))\n",
    "\n",
    "\n",
    "def frontiere_3d(f, data, step=20):\n",
    "    \"\"\"Plot the 3d frontiere for the decision function f.\"\"\"\n",
    "    ax = plt.gca(projection='3d')\n",
    "    xmin, xmax = data[:, 0].min() - 1., data[:, 0].max() + 1.\n",
    "    ymin, ymax = data[:, 1].min() - 1., data[:, 1].max() + 1.\n",
    "    xx, yy = np.meshgrid(np.arange(xmin, xmax, (xmax - xmin) * 1. / step),\n",
    "                         np.arange(ymin, ymax, (ymax - ymin) * 1. / step))\n",
    "    z = np.array([f(vec) for vec in np.c_[xx.ravel(), yy.ravel()]])\n",
    "    z = z.reshape(xx.shape)\n",
    "    ax.plot_surface(xx, yy, z, rstride=1, cstride=1,\n",
    "                    linewidth=0., antialiased=False,\n",
    "                    cmap=plt.cm.coolwarm)\n",
    "\n",
    "\n",
    "def plot_cout(X, y, loss_fun, w=None):\n",
    "    \"\"\"Plot the cost function encoded by loss_fun,\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : data features\n",
    "    y :  labels\n",
    "    loss_fun : loss function\n",
    "    w : (optionnal) can be used to give a historic path of the weights \"\"\"\n",
    "    def _inter(wn):\n",
    "        ww = np.zeros(3)\n",
    "        ww[1:] = wn\n",
    "        return loss_fun(X, y, ww).mean()\n",
    "    datarange = np.array([[np.min(X[:, 0]), np.min(X[:, 1])],\n",
    "                          [np.max(X[:, 0]), np.max(X[:, 1])]])\n",
    "    frontiere(_inter, np.array(datarange))\n",
    "    if w is not None:\n",
    "        plt.plot(w[:, 1], w[:, 2], 'k')\n",
    "    plt.xlim([np.min(X[:, 0]), np.max(X[:, 0])])\n",
    "    plt.ylim([np.min(X[:, 1]), np.max(X[:, 1])])\n",
    "\n",
    "\n",
    "def plot_cout3d(x, y, loss_fun, w):\n",
    "    \"\"\" trace le cout de la fonction cout loss_fun passee en parametre, en x,y,\n",
    "        en faisant varier les coordonnees du poids w.\n",
    "        W peut etre utilise pour passer un historique de poids\"\"\"\n",
    "    def _inter(wn):\n",
    "        ww = np.zeros(3)\n",
    "        ww[1:] = wn\n",
    "        return loss_fun(x, y, ww).mean()\n",
    "\n",
    "    datarange = np.array([[w[:, 1].min(), w[:, 2].min()],\n",
    "                         [w[:, 1].max(), w[:, 2].max()]])\n",
    "    frontiere_3d(_inter, np.array(datarange))\n",
    "    plt.plot(w[:, 1], w[:, 2], np.array([_inter(w[i, 1:]) for i in\n",
    "             range(w.shape[0])]), 'k-', linewidth=3)\n",
    "\n",
    "###############################################################################\n",
    "#                Algorithms and functions\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def gradient(x, y, eps, niter, w_ini, loss_fun, gr_loss_fun, stochastic=True):\n",
    "    \"\"\" algorithme de descente du gradient:\n",
    "        - x : donnees\n",
    "        - y : label\n",
    "        - eps : facteur multiplicatif de descente\n",
    "        - niter : nombre d'iterations\n",
    "        - w_ini\n",
    "        - loss_fun : fonction de cout\n",
    "        - gr_loss_fun : gradient de la fonction de cout\n",
    "        - stoch : True : gradient stochastique\n",
    "        \"\"\"\n",
    "    w = np.zeros((niter, w_ini.size))\n",
    "    w[0] = w_ini\n",
    "    loss = np.zeros(niter)\n",
    "    loss[0] = loss_fun(x, y, w[0]).mean()\n",
    "    for i in range(1, niter):\n",
    "        if stochastic:  # this is for Stochastic Gradient Descent\n",
    "            idx = [np.random.randint(x.shape[0])]\n",
    "        else:           # this is for pure Gradient Descent\n",
    "            idx = np.arange(x.shape[0])\n",
    "        w[i, :] = w[i - 1, :] - eps * gr_loss_fun(x[idx, :],\n",
    "                                                  y[idx], w[i - 1, :])\n",
    "        loss[i] = loss_fun(x, y, w[i, :]).mean()\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "def plot_gradient(X, y, wh, cost_hist, loss_fun):\n",
    "    \"\"\" display 4 figures on how  (stochastic) gradient descent behaves\n",
    "    wh : solution history\n",
    "    cost_hist : cost history\n",
    "    loss_fun : loss function\n",
    "    \"\"\"\n",
    "    best = np.argmin(cost_hist)\n",
    "    plt.subplot(221)\n",
    "    plt.title('Data and hyperplane estimated')\n",
    "    plot_2d(X, y, wh[best, :])\n",
    "    plt.subplot(222)\n",
    "    plt.title('Projection of level line and algorithm path')\n",
    "    plot_cout(X, y, loss_fun, wh)\n",
    "    plt.subplot(223)\n",
    "    plt.title('Objective function vs iterations')\n",
    "    plt.plot(range(cost_hist.shape[0]), cost_hist)\n",
    "    plt.subplot(224, projection='3d')\n",
    "    plt.title('Level line and algorithm path')\n",
    "    plot_cout3d(X, y, loss_fun, wh)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "#                Polynomial transformations\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def poly2(x):\n",
    "    \"\"\" creates features for second order interactions \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    nb, d = x.shape\n",
    "    res = x\n",
    "    for i in range(0, d):\n",
    "        for j in range(i, d):\n",
    "            res = np.hstack((res, x[:, i:i + 1] * x[:, j:j + 1]))\n",
    "    return res\n",
    "\n",
    "\n",
    "def poly3(x):\n",
    "    \"\"\" creates features for third order interactions \"\"\"\n",
    "    if x.ndim == 1:\n",
    "            x = x[None, :]\n",
    "    nb, d = x.shape\n",
    "    res = poly2(x)\n",
    "    for i in range(0, d):\n",
    "        for j in range(i, d):\n",
    "            for k in range(j, d):\n",
    "                res = np.hstack(\n",
    "                    (res, x[:, i:i + 1] * x[:, j:j + 1] * x[:, k:k + 1]))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Génération artificielle de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s1 = rand_gauss()\n",
    "s2 = rand_bi_gauss()\n",
    "s3 = rand_clown()\n",
    "s4 = rand_checkers()\n",
    "\n",
    "X = s2[0]\n",
    "y = s2[1]\n",
    "\n",
    "plot_2d(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les classifieurs linéaires (affines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1., -1.,  1., -1.,  1., -1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "       -1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,  1., -1.,\n",
       "        1., -1.,  1., -1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
       "       -1., -1., -1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.,  1.,  1.,\n",
       "        1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1.,\n",
       "        1.,  1., -1.,  1., -1., -1.,  1.,  1.,  1., -1.,  1., -1.,  1.,\n",
       "       -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,\n",
       "       -1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,\n",
       "        1., -1., -1.,  1.,  1., -1.,  1.,  1., -1., -1., -1.,  1., -1.,\n",
       "        1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,\n",
       "       -1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1.,\n",
       "       -1.,  1., -1., -1., -1., -1., -1.,  1., -1., -1., -1.,  1.,  1.,\n",
       "        1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.,  1., -1.,  1.,\n",
       "       -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1., -1.,  1.,  1.,  1.,\n",
       "        1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1., -1., -1.,\n",
       "        1.,  1., -1.,  1., -1.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [0,1,2]\n",
    "eps = 0.01\n",
    "\n",
    "\n",
    "resultat = gradient(X,y,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
